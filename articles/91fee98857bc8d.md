---
title: "多重実行を防ぐためにGoでクラウド環境での排他制御を考える"
emoji: "🐯"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["golang"]
published: false
---

# はじめに

## TL;DR
- クラウド環境では分散ロックにする。ローカルでは起きないが、本番環境で起きるという排他制御の問題を回避できる。


#　背景

よくウェブサービスの勉強をしている時、クイックスタートを読んで写経していたが、排他制御に対して全くと言っていいほど触れていない。

```go
func main() {
	r := gin.Default() // デフォルトの Gin エンジン (ミドルウェア付き)
	r.GET("/ping", func(c *gin.Context) { // GET /ping ハンドラ
		c.JSON(200, gin.H{"message": "pong"})
	})
	r.Run() // サーバ起動
}
```

同時実行に対してどうするんだろうという疑問。


# Mutexを導入

Mutexを導入すればいい。

```go
package service

import (
	"sync"
	"sync/atomic"
)

var globalLock = &lockManager{}

type lockRef struct {
	mu    sync.Mutex
	count int32
}

func newLockRef() *lockRef {
	return &lockRef{
		count: 1,
	}
}

// LockManager ユニークIDごとにロックを管理する構造体
type lockManager struct {
	locks sync.Map
}

type LockManager interface {
	Lock(uid string)
	Unlock(uid string)
}

func NewLockManager() LockManager {
	return globalLock
}

// Lock ユニークIDに基づいてロックを獲得する
func (lm *lockManager) Lock(uid string) {
	for {
		actual, loaded := lm.locks.LoadOrStore(uid, newLockRef())
		lRef := actual.(*lockRef)
		if loaded {
			// 既存の場合は参照カウントをチェックし、加算する
			if atomic.LoadInt32(&lRef.count) == 0 {
				continue
			}
			atomic.AddInt32(&lRef.count, 1)
		}
		lRef.mu.Lock()
		break
	}
}

// Unlock ロックを解放し、参照カウントが0になったらマップから削除
func (lm *lockManager) Unlock(uid string) {
	if value, ok := lm.locks.Load(uid); ok {
		lRef := value.(*lockRef)
		lRef.mu.Unlock()
		if atomic.AddInt32(&lRef.count, -1) == 0 {
			lm.locks.Delete(uid)
		}
	}
}
```

#　問題点

我ながらよくできた。
ローカルで排他制御ができて安心していた。

しかし、気づいた。
これは同一プロセス内でしか、排他制御ができていない。

昨今はクラウド環境にデプロイすることが多い。
その際、水平スケールをされるとこれは排他できない。

このコードは同一ユーザーのログインセッション内の多重実行を防いでくれるだけで、バッチ処理と重なったり、複数端末のログインで簡単に崩壊してしまう。


# 分散ロックを導入する
まずは、redisを使います。

```go
package service

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/hololive/holoearth-ugc-backend/infrastructure/driver"
)

// LockManager はユニークIDごとにロックを管理する構造体
type lockManager struct {
	client Redsync
}

//go:generate mockgen -source=$GOFILE -destination=./mock/mock_$GOFILE -package mock
type LockManager interface {
	LockWithContext(ctx context.Context, uid string) (func(), error)
}

func NewLockManager(redsyncClient driver.RedisDriver) LockManager {
	return &lockManager{
		redsyncClient: redsyncClient,
	}
}

// LockWithContext
func (l *lockManager) LockWithContext(ctx context.Context, uid string) (func(), error) {
	if uid == "" {
		return nil, fmt.Errorf("uid is empty")
	}

	lockKey := "user-lock:" + uid

	mutex, err := l.client.Lock(ctx, lockKey)
	if err != nil {
		return nil, err
	}

	// unlock関数を返す。クリーンアップ関数を用意して、呼び出し元にdeferで読んでもらう。
	unlockFunc := func() {
		err := l.client.UnLock(context.Background(), mutex)
        if err != nil {
			slog.Warn("unlock failed", "error", err, "uid", uid)
		}
	}

	return unlockFunc, nil
}

```

```go

type Redsync interface {
	Lock(ctx context.Context, key string, tries, ttl int) (*redsync.Mutex, error)
	UnLock(ctx context.Context, mutexlist *redsync.Mutex) error
}


type redsync struct {
	rs        *redsync.Redsync
}

func (r *redsync) Lock(ctx context.Context, key string) (*redsync.Mutex, error) {
	mutex := r.rs.NewMutex(key)
	err := mutex.LockContext(ctx)
	if err != nil {
		return nil, err
	}
	return mutex, nil
}

func (r *redsync) UnLock(ctx context.Context, mutex *redsync.Mutex) error {
    ok, err := mutex.UnlockContext(ctx)
    if err != nil {
        return err
    }
    if !ok {
        return fmt.Errorf("unlock failed: not owner of lock")
    }
    return nil
}

```